{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c379e83a",
   "metadata": {},
   "source": [
    "# <span style=\"color:red; font-weight:bold; \">A clean and modern RangeViT implementation for SemanticKITTI in PyTorch 2.4</span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85e44a8",
   "metadata": {},
   "source": [
    "## <span style=\"font-weight:bold\">1. DataLoader</span>\n",
    "\n",
    "### 1.1 Dataset Structure\n",
    "The dataset should be structured as follows:\n",
    "```\n",
    "sequences/\n",
    "├── 03/\n",
    "│   ├── velodyne/\n",
    "│   │   ├── 000000.bin\n",
    "│   │   ├── 000001.bin\n",
    "│   ├── labels/\n",
    "│   │   ├── 000000.label\n",
    "│   │   ├── 000001.label\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8a4b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Projection\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class ScanProjection(object):\n",
    "    '''\n",
    "    Project the 3D point cloud to 2D data with range projection\n",
    "\n",
    "    Adapted from A. Milioto et al. https://github.com/PRBonn/lidar-bonnetal\n",
    "    '''\n",
    "\n",
    "    def __init__(self, proj_w, proj_h):\n",
    "        # params of proj img size\n",
    "        self.proj_w = proj_w\n",
    "        self.proj_h = proj_h\n",
    "\n",
    "\n",
    "    def doProjection(self, pointcloud: np.ndarray, label: np.array):\n",
    "        # make sure pointcloud and label are the same length\n",
    "        assert pointcloud.shape[0] == label.shape[0], \"Pointcloud and label must have the same number of points\"\n",
    "        # get depth of all points\n",
    "        depth = np.linalg.norm(pointcloud[:, :3], 2, axis=1)\n",
    "        # get point cloud components\n",
    "        x = pointcloud[:, 0]\n",
    "        y = pointcloud[:, 1]\n",
    "        z = pointcloud[:, 2]\n",
    "\n",
    "        # get angles of all points\n",
    "        yaw = -np.arctan2(y, -x)\n",
    "        proj_x = 0.5 * (yaw / np.pi + 1.0)  # in [0.0, 1.0]\n",
    "        #breakpoint()\n",
    "        new_raw = np.nonzero((proj_x[1:] < 0.2) * (proj_x[:-1] > 0.8))[0] + 1\n",
    "        proj_y = np.zeros_like(proj_x)\n",
    "        proj_y[new_raw] = 1\n",
    "        proj_y = np.cumsum(proj_y)\n",
    "        # scale to image size using angular resolution\n",
    "        proj_x = proj_x * self.proj_w - 0.001\n",
    "\n",
    "        # round and clamp for use as index\n",
    "        proj_x = np.maximum(np.minimum(\n",
    "            self.proj_w - 1, np.floor(proj_x)), 0).astype(np.int32)\n",
    "\n",
    "        proj_y = np.maximum(np.minimum(\n",
    "            self.proj_h - 1, np.floor(proj_y)), 0).astype(np.int32)\n",
    "\n",
    "        # order in decreasing depth\n",
    "        indices = np.arange(depth.shape[0])\n",
    "        order = np.argsort(depth)[::-1]\n",
    "        depth = depth[order]\n",
    "        indices = indices[order]\n",
    "        pointcloud = pointcloud[order]\n",
    "        proj_y = proj_y[order]\n",
    "        proj_x = proj_x[order]\n",
    "        label = label[order]\n",
    "\n",
    "        # get projection result\n",
    "        proj_range = np.full((self.proj_h, self.proj_w), -1, dtype=np.float32)\n",
    "        proj_range[proj_y, proj_x] = depth\n",
    "\n",
    "        proj_pointcloud = np.full((self.proj_h, self.proj_w, pointcloud.shape[1]), -1, dtype=np.float32)\n",
    "        proj_pointcloud[proj_y, proj_x] = pointcloud\n",
    "\n",
    "        proj_idx = np.full((self.proj_h, self.proj_w), -1, dtype=np.int32)\n",
    "        proj_idx[proj_y, proj_x] = indices\n",
    "\n",
    "        proj_label = np.full((self.proj_h, self.proj_w), 0, dtype=np.int32)\n",
    "        proj_label[proj_y, proj_x] = label\n",
    "\n",
    "        # create proj_tensor with cascade proj_pointcloud and proj_range\n",
    "        # proj_pointcloud has size (64, 2048, 4)\n",
    "        # proj_range has size (64, 2048)\n",
    "        proj_tensor = np.concatenate((proj_pointcloud, proj_range[..., np.newaxis]), axis=-1)\n",
    "        return proj_tensor, proj_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5203ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DataLoader\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class KITTISegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.pc_dir = os.path.join(root_dir, 'velodyne')\n",
    "        self.label_dir = os.path.join(root_dir, 'labels')\n",
    "        # Get the list of files (no extension) in the point cloud directory\n",
    "        self.file_list = [f[:-4] for f in os.listdir(self.pc_dir) if f.endswith('.bin')]\n",
    "        # Setup the projection parameters\n",
    "        self.projection = ScanProjection(proj_w=2048, proj_h=64)\n",
    "        # Define the learning map for semantic labels\n",
    "        # This map is used to convert the original labels to a smaller set of classes\n",
    "        self.learning_map = {0: 0, 1: 0, 10: 1, 11: 2, 13: 5, 15: 3, 16: 5, 18: 4, 20: 5,\n",
    "            30: 6, 31: 7, 32: 8, 40: 9, 44: 10, 48: 11, 49: 12, 50: 13,\n",
    "            51: 14, 52: 0, 60: 9, 70: 15, 71: 16, 72: 17, 80: 18, 81: 19,\n",
    "            99: 0, 252: 1, 253: 7, 254: 6, 255: 8, 256: 5, 257: 5, 258: 4, 259: 5}\n",
    "        # Create a mapping array with size large enough to cover the largest key\n",
    "        self.max_key = max(self.learning_map.keys())\n",
    "        self.map_array = np.zeros((self.max_key + 1,), dtype=np.int32)\n",
    "        # Fill the mapping array with the learning map values\n",
    "        for key, value in self.learning_map.items():\n",
    "            self.map_array[key] = value\n",
    "            \n",
    "    # Read the point cloud data from binary files\n",
    "    @staticmethod\n",
    "    def readPCD(path):\n",
    "        pcd = np.fromfile(path, dtype=np.float32).reshape(-1, 4) # 4 channels: x, y, z, intensity\n",
    "        return pcd\n",
    "    \n",
    "    # Read the label data from files\n",
    "    @staticmethod\n",
    "    def readLabel(path):\n",
    "        label = np.fromfile(path, dtype=np.int32)\n",
    "        sem_label = label & 0xFFFF  # semantic label in lower half\n",
    "        inst_label = label >> 16  # instance id in upper half\n",
    "        return sem_label, inst_label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    @staticmethod\n",
    "    def proj(pc): # perform projection here\n",
    "        return pc\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.file_list[idx]\n",
    "        pc_path = os.path.join(self.pc_dir, f\"{fname}.bin\")\n",
    "        label_path = os.path.join(self.label_dir, f\"{fname}.label\")\n",
    "\n",
    "        # Load binary data\n",
    "        pc = self.readPCD(pc_path)  # x, y, z, intensity\n",
    "        label,_ = self.readLabel(label_path)  # shape [H, W]\n",
    "        # Map the labels using the learning map\n",
    "        label = self.map_array[label]  # map to smaller set of classes\n",
    "        img, label = self.projection.doProjection(pc, label) # shape [H, W, C]\n",
    "        img = torch.tensor(img).permute(2, 0, 1).float()  # to [C, H, W]\n",
    "        label = torch.tensor(label).long()                # [H, W]\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace32aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "dataset = KITTISegmentationDataset('../SemanticKITTI/dataset/sequences/07')\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727ccd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import timm\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class RangeViTSegmentationModel(nn.Module):\n",
    "#     def __init__(self, in_channels, num_classes):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Step 1: Load base ViT\n",
    "#         self.backbone = timm.create_model(\n",
    "#             'vit_small_patch16_384',\n",
    "#             pretrained=True,\n",
    "#             in_chans=in_channels,\n",
    "#             features_only=False\n",
    "#         )\n",
    "\n",
    "#         # Step 2: Replace the patch embedding (stem) with custom one\n",
    "#         self.backbone.patch_embed = nn.Conv2d(\n",
    "#             in_channels=in_channels,\n",
    "#             out_channels=self.backbone.embed_dim,\n",
    "#             kernel_size=(2, 8),\n",
    "#             stride=(2, 8)\n",
    "#         )\n",
    "#         num_patches = (64 // 2) * (384 // 8)  # 1536\n",
    "#         self.backbone.pos_embed = nn.Parameter(torch.zeros(1, 1 + num_patches, self.backbone.embed_dim))\n",
    "#         nn.init.trunc_normal_(self.backbone.pos_embed, std=0.02)\n",
    "#         self.backbone.cls_token = nn.Parameter(torch.zeros(1, 1, self.backbone.embed_dim))\n",
    "#         nn.init.trunc_normal_(self.backbone.cls_token, std=0.02)\n",
    "#         # Step 3: Optional — update positional embeddings\n",
    "#         self.update_pos_embed(new_hw=(32, 48))  # since 64/2, 384/8\n",
    "\n",
    "#         # Step 4: Segmentation head\n",
    "#         self.seg_head = nn.Sequential(\n",
    "#             nn.Conv2d(self.backbone.embed_dim, 256, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "#         )\n",
    "\n",
    "#         self.original_size = None\n",
    "\n",
    "#     def update_pos_embed(self, new_hw):\n",
    "#         \"\"\"Resize positional embedding to match new patch layout (new_hw = (H, W))\"\"\"\n",
    "#         num_patches = new_hw[0] * new_hw[1]\n",
    "#         old_pos_embed = self.backbone.pos_embed  # shape [1, 1 + N, D]\n",
    "#         cls_token = old_pos_embed[:, :1, :]      # shape [1, 1, D]\n",
    "#         grid_embed = old_pos_embed[:, 1:, :]     # shape [1, N, D]\n",
    "\n",
    "#         # Infer original grid size\n",
    "#         N_old = grid_embed.shape[1]\n",
    "#         D = grid_embed.shape[2]\n",
    "#         h_old = w_old = int(N_old ** 0.5)\n",
    "#         # assert h_old * w_old == N_old, \"Non-square original grid\"\n",
    "\n",
    "#         # Reshape to 2D spatial grid and interpolate\n",
    "#         grid_embed = grid_embed.reshape(1, h_old, w_old, D).permute(0, 3, 1, 2)  # [1, D, h_old, w_old]\n",
    "#         grid_embed = F.interpolate(grid_embed, size=new_hw, mode='bilinear', align_corners=False)  # [1, D, H, W]\n",
    "#         grid_embed = grid_embed.permute(0, 2, 3, 1).reshape(1, new_hw[0] * new_hw[1], D)  # [1, H*W, D]\n",
    "#         print(\"cls_token shape:\", cls_token.shape)      # should be [1, 1, D]\n",
    "#         print(\"grid_embed shape:\", grid_embed.shape)    # should be [1, N, D]\n",
    "#         # Concatenate back with cls_token\n",
    "#         new_pos_embed = torch.cat([cls_token, grid_embed], dim=1)  # [1, 1 + H*W, D]\n",
    "#         self.backbone.pos_embed = nn.Parameter(new_pos_embed)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         self.original_size = x.shape[2:]\n",
    "#         print(f\"Input shape: {x.shape}\")\n",
    "#         x_resized = F.interpolate(x, size=(64, 384), mode='bilinear', align_corners=False)\n",
    "#         print(f\"Resized input shape: {x_resized.shape}\")\n",
    "\n",
    "#         feats = self.backbone(x_resized)\n",
    "#         print(f\"feats shape before reshape: {feats.shape}\")\n",
    "#         B = x_resized.shape[0]\n",
    "#         h = 64 // 2   # patch height\n",
    "#         w = 384 // 8  # patch width\n",
    "#         C = feats.shape[-1]\n",
    "       \n",
    "#         feats = feats[:, 1:, :].reshape(B, h, w, C).permute(0, 3, 1, 2)\n",
    "\n",
    "#         logits = self.seg_head(feats)\n",
    "#         return F.interpolate(logits, size=self.original_size, mode='bilinear', align_corners=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58cd3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RangeViTSegmentationModel(nn.Module):\n",
    "    def __init__(self, in_channels, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create ViT model without features_only to see what we actually get\n",
    "        self.backbone = timm.create_model(\n",
    "            'vit_small_patch16_384',       \n",
    "            pretrained=True,\n",
    "            in_chans=in_channels,\n",
    "            num_classes=0,  # Set num_classes to 0 to avoid classification head; this has no effect on number of classes in seg_head (still 20)\n",
    "            global_pool='', # disables CLS token pooling\n",
    "            features_only=False  # Don't use features_only\n",
    "        )\n",
    "        \n",
    "        # Get the actual feature dimension from the model\n",
    "        feat_dim = 384  # This should be 384 for vit_small\n",
    "        hidden_dim = 256\n",
    "        # Print for debugging\n",
    "        print(f\"ViT feature dimension: {feat_dim}\")\n",
    "        print(f\"number of classes: {n_classes}\")\n",
    "        # Create segmentation head with the correct input dimension\n",
    "        self.seg_head = nn.Sequential(\n",
    "            nn.Conv2d(feat_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, n_classes, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "        self.original_size = None  # Store original size for resizing back\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Store original size for later upsampling\n",
    "        self.original_size = x.shape[2:]\n",
    "        \n",
    "        # print for debuggings\n",
    "        # print(f\"Input shape: {x.shape}\")\n",
    "        # Resize input to 384x384 (what ViT expects)\n",
    "        x_resized = F.interpolate(x, size=(384, 384), mode='bilinear', align_corners=False)\n",
    "        # print(f\"Resized input shape: {x_resized.shape}\")\n",
    "\n",
    "        # Extract features from backbone\n",
    "        feats = self.backbone(x_resized)\n",
    "        # print(f\"Features shape: {feats.shape}\")\n",
    "        # Reshape features for segmentation head \n",
    "        # ViT returns tokens, we need to reshape to 2D feature map\n",
    "        B = x_resized.shape[0]\n",
    "        h = w = int(384 / 16)  # 16 is patch size of vit_small_patch16_384\n",
    "        C = feats.shape[-1]\n",
    "        # print(f\"Reshaping features to: [B, C, h, w] = [{B}, {C}, {h}, {w}]\")\n",
    "        # Remove CLS token and reshape to [B, C, h, w]\n",
    "        feats = feats[:, 1:, :].reshape(B, h, w, C).permute(0, 3, 1, 2)\n",
    "        \n",
    "        # Apply segmentation head\n",
    "        logits = self.seg_head(feats)\n",
    "        \n",
    "        # Resize back to original dimensions\n",
    "        return F.interpolate(logits, size=self.original_size, mode='bilinear', align_corners=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b344189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def compute_iou(preds, labels, num_classes, ignore_index=None):\n",
    "    \"\"\"\n",
    "    preds: [B, H, W] — predicted class indices\n",
    "    labels: [B, H, W] — ground-truth class indices\n",
    "    \"\"\"\n",
    "    ious = []\n",
    "    for cls in range(num_classes):\n",
    "        if ignore_index is not None and cls == ignore_index:\n",
    "            continue\n",
    "\n",
    "        pred_cls = (preds == cls)\n",
    "        label_cls = (labels == cls)\n",
    "\n",
    "        intersection = (pred_cls & label_cls).sum().float()\n",
    "        union = (pred_cls | label_cls).sum().float()\n",
    "\n",
    "        if union == 0:\n",
    "            ious.append(torch.tensor(float('nan')))  # skip class if no samples\n",
    "        else:\n",
    "            ious.append(intersection / union)\n",
    "\n",
    "    # Compute mean ignoring NaNs\n",
    "    ious = torch.stack(ious)\n",
    "    mIoU = torch.nanmean(ious).item()\n",
    "    return mIoU, ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the model\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = 20\n",
    "in_channels = 5 # range, x, y, z, intensity\n",
    "model = RangeViTSegmentationModel(n_classes=num_classes, in_channels=in_channels).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# example dataloader from previous step\n",
    "for epoch in range(1):  # or more\n",
    "    model.train()\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device)                # [B, C, H, W]\n",
    "        labels = labels.to(device).long()     # [B, H, W]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)                 # [B, num_classes, H, W]\n",
    "        loss = criterion(outputs, labels)\n",
    "        preds = outputs.argmax(dim=1)         # [B, H, W]\n",
    "        mIoU, ious = compute_iou(preds, labels, num_classes, ignore_index=0)  # ignore background class\n",
    "        print(f\"mIoU: {mIoU:.4f}\")\n",
    "        print(f\"Loss: {loss.item():.4f}\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}] Loss: {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cleanViT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
